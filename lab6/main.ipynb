{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T09:35:53.685673Z",
     "start_time": "2024-12-27T09:35:53.680236Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ],
   "id": "f248df720aab48ea",
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T09:35:53.719256Z",
     "start_time": "2024-12-27T09:35:53.715651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class CharacterRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size, num_layers=1):\n",
    "        super(CharacterRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        \n",
    "        # Define the RNN layer\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # Output layer\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "    \n",
    "    def forward(self, x, hidden):\n",
    "        # Forward pass through RNN\n",
    "        out, hidden = self.rnn(x, hidden)\n",
    "        # Pass through final layer\n",
    "        out = self.fc(out)\n",
    "        return out, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        # Initialize hidden state\n",
    "        return torch.zeros(self.num_layers, batch_size, self.hidden_size)"
   ],
   "id": "8480760481a47fb3",
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T09:35:53.735942Z",
     "start_time": "2024-12-27T09:35:53.724374Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class TextGenerator:\n",
    "    def __init__(self, text, sequence_length=6):\n",
    "        self.text = text\n",
    "        self.sequence_length = sequence_length\n",
    "        self.chars = sorted(list(set(text)))\n",
    "        self.char_to_idx = {char: i for i, char in enumerate(self.chars)}\n",
    "        self.idx_to_char = {i: char for i, char in enumerate(self.chars)}\n",
    "        self.vocab_size = len(self.chars)\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"Create training sequences and corresponding targets\"\"\"\n",
    "        sequences = []\n",
    "        next_chars = []\n",
    "        \n",
    "        for i in range(0, len(self.text) - self.sequence_length):\n",
    "            sequences.append(self.text[i:i + self.sequence_length])\n",
    "            next_chars.append(self.text[i + self.sequence_length])\n",
    "        \n",
    "        # Convert to one-hot encoded vectors\n",
    "        X = np.zeros((len(sequences), self.sequence_length, self.vocab_size))\n",
    "        y = np.zeros((len(sequences), self.vocab_size))\n",
    "        \n",
    "        for i, sequence in enumerate(sequences):\n",
    "            for t, char in enumerate(sequence):\n",
    "                X[i, t, self.char_to_idx[char]] = 1\n",
    "            y[i, self.char_to_idx[next_chars[i]]] = 1\n",
    "        \n",
    "        return torch.FloatTensor(X), torch.FloatTensor(y)\n",
    "    \n",
    "    def train_model(self, hidden_size=128, num_layers=1, num_epochs=100, batch_size=128, learning_rate=0.001):\n",
    "        \"\"\"Train the RNN model\"\"\"\n",
    "        X, y = self.prepare_data()\n",
    "        \n",
    "        model = CharacterRNN(self.vocab_size, hidden_size, self.vocab_size, num_layers)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "        \n",
    "        # Training loop\n",
    "        for epoch in range(num_epochs):\n",
    "            model.zero_grad()\n",
    "            hidden = model.init_hidden(X.size(0))\n",
    "            \n",
    "            output, hidden = model(X, hidden)\n",
    "            loss = criterion(output[:, -1, :], torch.max(y, 1)[1])\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            if (epoch + 1) % 10 == 0:\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    def generate_text(self, model, seed_text, length=100, temperature=0.5):\n",
    "        \"\"\"Generate text using the trained model with fixed length seed text\"\"\"\n",
    "        # Проверка длины seed_text\n",
    "        if len(seed_text) != self.sequence_length:\n",
    "            raise ValueError(f\"Seed text must be exactly {self.sequence_length} characters long\")\n",
    "        \n",
    "        # Проверка символов в словаре\n",
    "        for char in seed_text:\n",
    "            if char not in self.char_to_idx:\n",
    "                raise ValueError(f\"Character '{char}' not found in training vocabulary\")\n",
    "        \n",
    "        model.eval()\n",
    "        current_sequence = seed_text\n",
    "        generated_text = seed_text\n",
    "        \n",
    "        # Generate new characters\n",
    "        for _ in range(length):\n",
    "            # Prepare input\n",
    "            x = np.zeros((1, self.sequence_length, self.vocab_size))\n",
    "            for t, char in enumerate(current_sequence):\n",
    "                x[0, t, self.char_to_idx[char]] = 1\n",
    "            x = torch.FloatTensor(x)\n",
    "            \n",
    "            # Forward pass\n",
    "            hidden = model.init_hidden(1)\n",
    "            output, hidden = model(x, hidden)\n",
    "            \n",
    "            # Apply temperature and get probabilities\n",
    "            output = output[0, -1, :] / temperature\n",
    "            probs = torch.softmax(output, dim=0).detach().numpy()\n",
    "            \n",
    "            # Sample next character\n",
    "            next_char_idx = np.random.choice(len(probs), p=probs)\n",
    "            next_char = self.idx_to_char[next_char_idx]\n",
    "            \n",
    "            # Update sequences\n",
    "            generated_text += next_char\n",
    "            current_sequence = current_sequence[1:] + next_char\n",
    "        \n",
    "        return generated_text\n",
    "\n"
   ],
   "id": "40164323644691a4",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T09:47:18.210860Z",
     "start_time": "2024-12-27T09:44:49.969731Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Инициализация с обучающим текстом\n",
    "with open('text.txt', 'r', encoding='utf-8') as file:\n",
    "    sample_text = file.read()\n",
    "\n",
    "# Создание и обучение модели\n",
    "generator = TextGenerator(sample_text, sequence_length=6)\n",
    "model = generator.train_model(num_epochs=1000)"
   ],
   "id": "7d254adfcea52b7f",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1000], Loss: 4.5528\n",
      "Epoch [20/1000], Loss: 3.9905\n",
      "Epoch [30/1000], Loss: 3.9403\n",
      "Epoch [40/1000], Loss: 3.9174\n",
      "Epoch [50/1000], Loss: 3.9011\n",
      "Epoch [60/1000], Loss: 3.8769\n",
      "Epoch [70/1000], Loss: 3.7958\n",
      "Epoch [80/1000], Loss: 3.6049\n",
      "Epoch [90/1000], Loss: 3.5251\n",
      "Epoch [100/1000], Loss: 3.4818\n",
      "Epoch [110/1000], Loss: 3.4413\n",
      "Epoch [120/1000], Loss: 3.3979\n",
      "Epoch [130/1000], Loss: 3.3480\n",
      "Epoch [140/1000], Loss: 3.2921\n",
      "Epoch [150/1000], Loss: 3.2316\n",
      "Epoch [160/1000], Loss: 3.1674\n",
      "Epoch [170/1000], Loss: 3.0964\n",
      "Epoch [180/1000], Loss: 3.0195\n",
      "Epoch [190/1000], Loss: 2.9385\n",
      "Epoch [200/1000], Loss: 2.8551\n",
      "Epoch [210/1000], Loss: 2.7690\n",
      "Epoch [220/1000], Loss: 2.6820\n",
      "Epoch [230/1000], Loss: 2.5962\n",
      "Epoch [240/1000], Loss: 2.5145\n",
      "Epoch [250/1000], Loss: 2.4377\n",
      "Epoch [260/1000], Loss: 2.3653\n",
      "Epoch [270/1000], Loss: 2.2979\n",
      "Epoch [280/1000], Loss: 2.2348\n",
      "Epoch [290/1000], Loss: 2.1759\n",
      "Epoch [300/1000], Loss: 2.1207\n",
      "Epoch [310/1000], Loss: 2.0696\n",
      "Epoch [320/1000], Loss: 2.0206\n",
      "Epoch [330/1000], Loss: 1.9741\n",
      "Epoch [340/1000], Loss: 1.9310\n",
      "Epoch [350/1000], Loss: 1.8883\n",
      "Epoch [360/1000], Loss: 1.8487\n",
      "Epoch [370/1000], Loss: 1.8101\n",
      "Epoch [380/1000], Loss: 1.7729\n",
      "Epoch [390/1000], Loss: 1.7375\n",
      "Epoch [400/1000], Loss: 1.7036\n",
      "Epoch [410/1000], Loss: 1.6713\n",
      "Epoch [420/1000], Loss: 1.6407\n",
      "Epoch [430/1000], Loss: 1.6108\n",
      "Epoch [440/1000], Loss: 1.5882\n",
      "Epoch [450/1000], Loss: 1.5573\n",
      "Epoch [460/1000], Loss: 1.5302\n",
      "Epoch [470/1000], Loss: 1.5043\n",
      "Epoch [480/1000], Loss: 1.4797\n",
      "Epoch [490/1000], Loss: 1.4587\n",
      "Epoch [500/1000], Loss: 1.4327\n",
      "Epoch [510/1000], Loss: 1.4108\n",
      "Epoch [520/1000], Loss: 1.3886\n",
      "Epoch [530/1000], Loss: 1.3725\n",
      "Epoch [540/1000], Loss: 1.3473\n",
      "Epoch [550/1000], Loss: 1.3279\n",
      "Epoch [560/1000], Loss: 1.3082\n",
      "Epoch [570/1000], Loss: 1.2888\n",
      "Epoch [580/1000], Loss: 1.2703\n",
      "Epoch [590/1000], Loss: 1.2518\n",
      "Epoch [600/1000], Loss: 1.2342\n",
      "Epoch [610/1000], Loss: 1.2201\n",
      "Epoch [620/1000], Loss: 1.2003\n",
      "Epoch [630/1000], Loss: 1.1844\n",
      "Epoch [640/1000], Loss: 1.1675\n",
      "Epoch [650/1000], Loss: 1.1519\n",
      "Epoch [660/1000], Loss: 1.1363\n",
      "Epoch [670/1000], Loss: 1.1211\n",
      "Epoch [680/1000], Loss: 1.1061\n",
      "Epoch [690/1000], Loss: 1.0918\n",
      "Epoch [700/1000], Loss: 1.0772\n",
      "Epoch [710/1000], Loss: 1.0633\n",
      "Epoch [720/1000], Loss: 1.0559\n",
      "Epoch [730/1000], Loss: 1.0370\n",
      "Epoch [740/1000], Loss: 1.0237\n",
      "Epoch [750/1000], Loss: 1.0105\n",
      "Epoch [760/1000], Loss: 0.9977\n",
      "Epoch [770/1000], Loss: 0.9853\n",
      "Epoch [780/1000], Loss: 0.9731\n",
      "Epoch [790/1000], Loss: 0.9680\n",
      "Epoch [800/1000], Loss: 0.9496\n",
      "Epoch [810/1000], Loss: 0.9388\n",
      "Epoch [820/1000], Loss: 0.9272\n",
      "Epoch [830/1000], Loss: 0.9160\n",
      "Epoch [840/1000], Loss: 0.9056\n",
      "Epoch [850/1000], Loss: 0.8945\n",
      "Epoch [860/1000], Loss: 0.8840\n",
      "Epoch [870/1000], Loss: 0.8736\n",
      "Epoch [880/1000], Loss: 0.8651\n",
      "Epoch [890/1000], Loss: 0.8536\n",
      "Epoch [900/1000], Loss: 0.8438\n",
      "Epoch [910/1000], Loss: 0.8351\n",
      "Epoch [920/1000], Loss: 0.8246\n",
      "Epoch [930/1000], Loss: 0.8169\n",
      "Epoch [940/1000], Loss: 0.8070\n",
      "Epoch [950/1000], Loss: 0.7970\n",
      "Epoch [960/1000], Loss: 0.7887\n",
      "Epoch [970/1000], Loss: 0.7805\n",
      "Epoch [980/1000], Loss: 0.7707\n",
      "Epoch [990/1000], Loss: 0.7643\n",
      "Epoch [1000/1000], Loss: 0.7544\n"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-27T09:48:31.210205Z",
     "start_time": "2024-12-27T09:48:31.187060Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Правильное использование: seed_text ровно 6 символов\n",
    "seed_text = \"button\"  # 6 символов\n",
    "generated = generator.generate_text(model, seed_text, length=100)\n",
    "print(f\"\\nСгенерированный текст:\\n{generated}\")"
   ],
   "id": "23e2d71fc170047b",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Сгенерированный текст:\n",
      "button onClick={toHtml:\n",
      "\n",
      "// src/TextEditorApi = {\n",
      "  p..\n",
      "\n",
      "    return currentState((currentState. seCSmate =\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# латентное представление\n",
    "# латентное пространство"
   ],
   "id": "e5b59d9f2b8eed98"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
